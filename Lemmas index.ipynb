{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an index of lemmas with Spark from any corpora\n",
    "\n",
    "The goal of this notebook is twofold. For another project I'm currently working on (the [Quickref](https://gitlab.common-lisp.net/quickref/quickref) project), I need a big list of various [lemmas](https://en.wikipedia.org/wiki/Lemma) but a classical list didn't fit my need. So I decide to develop this tool (currently at notebook state) to create the list of the most common lemmas of a corpus. I choose Spark to do it, first of all because I have big corpora and secondly because I want to practice Scala and Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I imagine this tool as a pipeline of transformation to get lemmas from a pool of file. Therefore, I'm considering the following architecture :\n",
    "1. list('path/to/files')\n",
    "2. map(path => content)\n",
    "3. flatMap(content => token)\n",
    "4. filter(token => clean_token)\n",
    "5. map_reduce(frequency of tokens) \n",
    "5. sort\n",
    "\n",
    "All these steps will be completly distributed with DataSet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import \n",
    "For this project, we will use some extra packages that we need to download/import. We will do it with the torree magic %AddJar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from http://dl.bintray.com/spark-packages/maven/databricks/spark-corenlp/0.3.1-s_2.11/spark-corenlp-0.3.1-s_2.11.jar\n",
      "Finished download of spark-corenlp-0.3.1-s_2.11.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar http://dl.bintray.com/spark-packages/maven/databricks/spark-corenlp/0.3.1-s_2.11/spark-corenlp-0.3.1-s_2.11.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from https://repo1.maven.org/maven2/edu/stanford/nlp/stanford-corenlp/3.9.1/stanford-corenlp-3.9.1-models.jar\n",
      "Finished download of stanford-corenlp-3.9.1-models.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar https://repo1.maven.org/maven2/edu/stanford/nlp/stanford-corenlp/3.9.1/stanford-corenlp-3.9.1-models.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import scala.io.Source\n",
    "import com.databricks.spark.corenlp.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkSession = org.apache.spark.sql.SparkSession@424d5745\n",
       "sc = org.apache.spark.SparkContext@6846c0b8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@6846c0b8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a spark session\n",
    "\n",
    "val sparkSession = SparkSession.builder\n",
    "                                    .appName(\"lemmas_index\")\n",
    "                                    .master(\"local\")\n",
    "                                    .getOrCreate()\n",
    "val sc = sparkSession.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to load my corpus. The corpus I will use in this notebook is constituted of all the READMEs of libraries referenced by [Quicklisp](https://www.quicklisp.org/beta/). I put it in the same directory of the notebook to facilitate this step. Because I don't think I have the right to upload this corpus on my repository, I will use it locally. Just replace the content of `corpus_directory` by the path of your own corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|./corpus/cl-grnm....|\n",
      "|./corpus/cl-cooki...|\n",
      "|./corpus/osicat.r...|\n",
      "|./corpus/oe-encod...|\n",
      "|./corpus/lastfm.r...|\n",
      "|./corpus/inferior...|\n",
      "|./corpus/asd-gene...|\n",
      "|./corpus/linewise...|\n",
      "|./corpus/listofli...|\n",
      "|./corpus/cl-scram...|\n",
      "|./corpus/definiti...|\n",
      "|./corpus/intel-he...|\n",
      "|./corpus/pooler.r...|\n",
      "|./corpus/lichat-t...|\n",
      "|./corpus/cl-steam...|\n",
      "|./corpus/lichat-s...|\n",
      "|./corpus/cl-olefs...|\n",
      "|./corpus/docutils...|\n",
      "|./corpus/ernestin...|\n",
      "|./corpus/png-read...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "corpus_directory = ./corpus\n",
       "pathsDS = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<console>:6: error: Symbol 'type scala.AnyRef' is missing from the classpath.\n",
       "This symbol is required by 'class org.apache.spark.sql.catalyst.QualifiedTableName'.\n",
       "Make sure that type AnyRef is in your classpath and check for conflicting dependencies with `-Ylog-classpath`.\n",
       "A full rebuild may help if 'QualifiedTableName.class' was compiled against an incompatible version of scala.\n",
       "  lazy val $print: String =  {\n",
       "           ^\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a DataSet from the list of files in corpus directory (converted into string)\n",
    "\n",
    "import java.io.File\n",
    "\n",
    "val corpus_directory = new File(\"./corpus/\")\n",
    "val pathsDS = corpus_directory.listFiles.filter(_.isFile).map(path => path.getPath()).toList.toDS()\n",
    "pathsDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|# cl-grnm\n",
      "\n",
      "Common...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "contentsDS = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read content of all files and replace path with it. We also need to remove empty document for further computations\n",
    "\n",
    "val contentsDS = pathsDS.map{path => {val source = Source.fromFile(path)\n",
    "                                      try source.mkString\n",
    "                                      finally source.close()}}.filter(c => c.length > 1)\n",
    "contentsDS.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Lemmatization\n",
    "Thanks to the spark-corenlp package, we can easily lemmatize our text. The lemma function given by spark-corenlp already tokenize sentences, so we can skip this step. When we have our token list lemmatized, we need to filter all the crap out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               lemma|\n",
      "+--------------------+\n",
      "|[#, cl-grnm, comm...|\n",
      "|[#, cl-cookie, -l...|\n",
      "|[-lsb-, !, -lsb-,...|\n",
      "|[*, introduction,...|\n",
      "|[#, lastfm, Inter...|\n",
      "|[INFERIOR-SHELL, ...|\n",
      "|[#, asd-generator...|\n",
      "|[linewise-templat...|\n",
      "|[*, overview, the...|\n",
      "|[#, cl-scram, ##,...|\n",
      "|[Project, 's, hom...|\n",
      "|[#, intel-hex, -,...|\n",
      "|[Pooler, ======, ...|\n",
      "|[##, about, Licha...|\n",
      "|[##, about, cl-st...|\n",
      "|[##, about, Licha...|\n",
      "|[#, +, title, :, ...|\n",
      "|[., ., -, *, -, r...|\n",
      "|[Ernestine, =====...|\n",
      "|[portable, Networ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawLemmaDS = [lemma: array<string>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[lemma: array<string>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Lemmatize contents but before, remove all empty rows\n",
    "\n",
    "val rawLemmaDS = contentsDS.select(lemma('value).as('lemma))\n",
    "rawLemmaDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|[cl-grnm, common-...|\n",
      "|[cl-cookie, build...|\n",
      "|[build, status, o...|\n",
      "|[introduction, th...|\n",
      "|[lastfm, interfac...|\n",
      "|[inferior-shell, ...|\n",
      "|[asd-generator, a...|\n",
      "|[linewise-templat...|\n",
      "|[overview, the, l...|\n",
      "|[cl-scram, introd...|\n",
      "|[project, home, d...|\n",
      "|[intel-hex, libra...|\n",
      "|[pooler, trivial,...|\n",
      "|[about, lichat-tc...|\n",
      "|[about, cl-steamw...|\n",
      "|[about, lichat-se...|\n",
      "|[title, cl-olef, ...|\n",
      "|[rst, cl-docutils...|\n",
      "|[ernestine, build...|\n",
      "|[portable, networ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lemmasByDocDS = [value: array<string>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<console>:57: warning: non-variable type argument String in type pattern Seq[String] (the underlying of Seq[String]) is unchecked since it is eliminated by erasure\n",
       "           case Row(tokens: Seq[String]) => tokens.filter(_.matches(\"^[A-Za-z][A-Za-z-]+[A-Za-z]$\")).map(_.toLowerCase)\n",
       "                            ^\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: array<string>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Filter all the crap\n",
    "val lemmasByDocDS = rawLemmaDS.map{\n",
    "    case Row(tokens: Seq[String]) => tokens.filter(_.matches(\"^[A-Za-z][A-Za-z-]+[A-Za-z]$\")).map(_.toLowerCase)\n",
    "}\n",
    "lemmasByDocDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|          value|\n",
      "+---------------+\n",
      "|        cl-grnm|\n",
      "|    common-lisp|\n",
      "| implementation|\n",
      "|            the|\n",
      "|grid-restrained|\n",
      "|            and|\n",
      "|    traditional|\n",
      "|    nelder-mead|\n",
      "|      algorithm|\n",
      "|     authorship|\n",
      "|           this|\n",
      "|        package|\n",
      "|     originally|\n",
      "|          write|\n",
      "|          mario|\n",
      "|         mommer|\n",
      "|            and|\n",
      "|           this|\n",
      "|           fork|\n",
      "|       maintain|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lemmasDS = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// transform DataSet[tokens: array<string>] into DataSet[token: string] by flatmaping it\n",
    "\n",
    "val lemmasDS = lemmasByDocDS.flatMap(_.iterator)\n",
    "lemmasDS.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regroup and sort it\n",
    "Now, the last step. We will regroup individual lemmas together, and then sort our dataset by number lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|           value|count|\n",
      "+----------------+-----+\n",
      "|      convergent|    3|\n",
      "|             few|  127|\n",
      "|            some|  896|\n",
      "|           input|  470|\n",
      "|           still|  207|\n",
      "|       recognize|   39|\n",
      "|           those|  185|\n",
      "|             art|    5|\n",
      "|           trail|   17|\n",
      "|       unlicense|   18|\n",
      "|        incoming|   14|\n",
      "|         degrade|    1|\n",
      "|          harder|    6|\n",
      "|          online|   23|\n",
      "|        priority|   75|\n",
      "|    evaluateable|    2|\n",
      "|     requirement|   49|\n",
      "|browser-specific|    1|\n",
      "|     interaction|   22|\n",
      "|         persist|   14|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lemmasCount = [value: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string, count: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Group by lemma and count individual lemma\n",
    "\n",
    "val lemmasCount = lemmasDS.groupBy('value).count()\n",
    "lemmasCount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|   value|count|\n",
      "+--------+-----+\n",
      "|     the|29883|\n",
      "|     and|10443|\n",
      "|     for| 6983|\n",
      "|     you| 5976|\n",
      "|     use| 5186|\n",
      "|    this| 4930|\n",
      "|function| 4757|\n",
      "|    that| 4503|\n",
      "|    lisp| 4180|\n",
      "|    with| 4140|\n",
      "|     can| 3624|\n",
      "|     not| 3392|\n",
      "|   value| 2972|\n",
      "|    name| 2938|\n",
      "|  return| 2690|\n",
      "|    will| 2610|\n",
      "|    list| 2446|\n",
      "|    have| 2380|\n",
      "|    from| 2360|\n",
      "|    test| 2200|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sortedLemmas = [value: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string, count: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// The last step is to sort all the lemmas !\n",
    "val sortedLemmas = lemmasCount.orderBy('count.desc)\n",
    "sortedLemmas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export\n",
    "Tada ! We have our new list of lemmas sorted by their frequency, extracted from a corpus ! That was not so hard ! To finalize this project, we just need to export it as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// First we need to coalesce all partitions into 1, and then we can output it in a single csv file\n",
    "\n",
    "sortedLemmas.coalesce(1).write.csv(\"./lemmas_frequency.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
